{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "\n",
    "## Introduction\n",
    "Say we have data which contains a set of variables and values associated with them in tabular form. Our objective is to guess the relation between a *target*($Y$) and set of *predictors*($ X_1, X_2, X_3, ... X_p $). \n",
    "\n",
    "We assume that the real world relation ship between $Y$ and $X = (X_1, X_2, ... , X_p)$ can be of the form\n",
    "$$ Y = f(X) + \\epsilon $$\n",
    "Where $f$ is some fixed but unknown function of $X$ and $\\epsilon$ is random *error term* which is independent of $X$ and has mean zero.\n",
    "\n",
    "## Motivation for Estimation of f\n",
    "We are interested in estimating $f$ for two reasons\n",
    "### Prediction\n",
    "Assume that we came up with estimate of $f$ as $\\hat{f}$ then the  resulting prediction using our estimate $\\hat{Y}$ can be generated as follows.\n",
    "$$\\hat{y} = \\hat{f}(X)$$\n",
    "Here we are not concerned with nature is $\\hat{f}$. We are mainly in for accuracy of $\\hat{Y}$\n",
    "The accuracy of $\\hat{Y}$ depends on two quantities *reducible error* and *irreducible error*. The reducible error is based on our selection of $\\hat{f}$ which can be minimized with better estimate $f$. But even we have right estimate for $f$ there is some error($\\epsilon$) which is independent  of $X$ and $f$. This is called irreducible error. The quantity $\\epsilon$ may also contain effect of unmeasured variable which isn't considered part of $X$.\n",
    "\n",
    "Assume our model didn't change and input X didn't change then $\\hat{Y}$ doesn't change but $Y$ changes as it is also dependent on error term. Then expectation of square error can be put as\n",
    "$$ E(Y - \\hat{Y})^2 = E[f(X) + \\epsilon - \\hat{f}(X)]^2$$\n",
    "$$ E(Y - \\hat{Y})^2 = E[f(X) - \\hat{f}(X)]^2 + E[e]^2 - 2(f(X) - \\hat{f})E[\\epsilon]$$\n",
    "$$ \\because E[x] = 0 $$\n",
    "$$ E(Y - \\hat{Y})^2 = E[f(X) - \\hat{f}(X)]^2 + E[e]^2$$\n",
    "\n",
    "The first part of result is known as *mean squared error* which is *reducible error*. The second part of the error term can be further solved to prove that it is *variance of* $\\epsilon$ which is *irreducible error*.\n",
    "\n",
    "### Inference\n",
    "In this case we are looking for relation ship between $X$ and $Y$ like.\n",
    "* Which predictors are associated with response\n",
    "* What is the relation ship between response and each predictor\n",
    "* Can these relationships be represented in linear fashion or some other complex curve. \n",
    "\n",
    "## Estimation of f\n",
    "### Parametric Estimation\n",
    "We make an assumption of model and try to guess the parameters using training data. In case of linear model we assume that\n",
    "$$ f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$\n",
    "Then using training data we try predict values of $\\beta_i$ so that the function fits our data better.\n",
    "$$ Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$\n",
    "\n",
    "This can be very useful for predicting relationships between $X_i$ and $Y$. But estimation accuracy largely depends on model we chose.\n",
    "### Non Parametric Estimation \n",
    "In case of non parametric estimation we decide value for predictors based on closest data points. This doesn't make any guesses on $f$ and can incorporate very complex patterns. But this requires huge set of training data to give accurate results.\n",
    "\n",
    "As we have observed there is a tradeoff between accuracy and interpretability of models. accuracy mainly depends on flexibility of the model. But more flexible the model the tougher it is to do the inference between predictors and target.\n",
    "\n",
    "## Assessing Model Accuracy\n",
    "Generally quality of fit is given by\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n",
    "There are two scenarios when we use $MSE$. *training* $MSE$ is used to fit the model to training data and *test* $MSE$ is used to validate the model once we trained it. \n",
    "\n",
    "We are looking for model which has lowest *test* $MSE$ instead of lowest *training* $MSE$. We naturally assume that the model with lowest *test* $MSE$ has the lowest *training* $MSE$ but unfortunately that's not the case.\n",
    "The common observed trend is as we increase the complexity of model *training* $MSE$ always drops while *test* $MSE$ drops till certain point and again increases.\n",
    "\n",
    "## Bias-Variance Trade-Off\n",
    "By extending from here\n",
    "$$ E(Y - \\hat{Y})^2 = E[f(X) - \\hat{f}(X)]^2 + E[e]^2$$\n",
    "we are interested in reducible error i.e $E[f(X) - \\hat{f}(X)]^2$ and see how it change with selection of model $\\hat{f}$\n",
    "\n",
    "$$ E[f(x) - \\hat{f}(X)]^2 = E[f(X)]^2 + E[\\hat{f}(X)]^2 - 2f(x)E(\\hat(f)(X))$$\n",
    "$$ E[f(x) - \\hat{f}(X)]^2 = f(X)^2 + var(\\hat{f}(X)) + (E(\\hat{f}(X)))^2 - 2f(x)E(\\hat(f)(X)) $$\n",
    "$$ E[f(x) - \\hat{f}(X)]^2 = var(\\hat{f}(X)) + (E(f(X) - \\hat{f}(X)))^2$$\n",
    "\n",
    "So the first component is known as *variance* and second component is known as *bias*\n",
    "\n",
    "The *variance* of model increases with increase in increase in complexity of model. \n",
    "The *bias* of model decreases with increase in complexity of model.\n",
    "\n",
    "Due to the interaction of these two the *test* $MSE$ behaves as mentioned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
