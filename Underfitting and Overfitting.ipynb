{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Overfitting and Underfitting\n",
    "\n",
    "## Introduction\n",
    "As models are built from training data they tend to follow the patterns that are specific to training data. But we may capture additional patterns that we not see in real world data or we may fail to capture necessary patterns that real world data expect.\n",
    "\n",
    "Over fitting is the case where our models capture the patterns that are only found in training data.\n",
    "Where as in under fitting we fail to capture expected pattern and our model gives inaccurate predictions\n",
    "\n",
    "The overfitting and underfitting is mainly decided on complication of model chosen.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"always\")\n",
    "\n",
    "# returns mean abslute error for given training, test data and a constraint on number of leaf nodes.\n",
    "def get_mae(max_leaf_nodes, train_X, val_x, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state = 0)\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_y = model.predict(val_X)\n",
    "    return mean_absolute_error(val_y, pred_y)\n",
    "\n",
    "file_path = \"data/mel_housing/melb_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# removing unclear rows\n",
    "data = data.dropna(0)\n",
    "\n",
    "# setting features and target. \n",
    "y = data.Price\n",
    "features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n",
    "                        'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "X = data[features]\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "Now we verify the test mean absolute error for different vales of max_leaf_nodes which decides depth of a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mae for max_leaf_nodes = 5    ',get_mae(5, train_X, val_X, train_y, val_y))\n",
    "print('mae for max_leaf_nodes = 50   ',get_mae(50, train_X, val_X, train_y, val_y))\n",
    "print('mae for max_leaf_nodes = 500  ',get_mae(500, train_X, val_X, train_y, val_y))\n",
    "print('mae for max_leaf_nodes = 5000 ',get_mae(5000, train_X, val_X, train_y, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As we can see from the experiment test mean absolute error first drops and again raises with increase in complexity of model.\n",
    "The errors happened due to simplicity of model(low max_leaf_nodes) is due to undefitting\n",
    "The errors happened due to following of patterns specific to train data(high max_leaf_nodes) is due to overfitting\n",
    "In above example max_leaf_nodes = 500 seems to be optimal among those values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
